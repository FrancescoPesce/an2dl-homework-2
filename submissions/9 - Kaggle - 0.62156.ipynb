{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10183992,"sourceType":"datasetVersion","datasetId":6291230}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artificial Neural Networks and Deep Learning - Homework 2\n","metadata":{"id":"nuwVgG3Vbbka"}},{"cell_type":"markdown","source":"## âš™ï¸ Import Libraries","metadata":{"id":"d7IqZP5Iblna"}},{"cell_type":"code","source":"!pip install focal-loss # default Keras focal loss does not support class weights ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:14.632128Z","iopub.execute_input":"2024-12-13T14:21:14.632629Z","iopub.status.idle":"2024-12-13T14:21:23.526139Z","shell.execute_reply.started":"2024-12-13T14:21:14.632599Z","shell.execute_reply":"2024-12-13T14:21:23.525222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\n\nimport matplotlib.pyplot as plt\n\nimport focal_loss\n\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nseed = 14\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\nprint(f\"TensorFlow version: {tf.__version__}\")\nprint(f\"Keras version: {tfk.__version__}\")\nprint(f\"GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")","metadata":{"id":"CO6_Ft_8T56A","outputId":"7ead05bc-0d0b-4c0c-80af-4c7e2d0f588e","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:46:08.480892Z","iopub.execute_input":"2024-12-13T14:46:08.481666Z","iopub.status.idle":"2024-12-13T14:46:08.523177Z","shell.execute_reply.started":"2024-12-13T14:46:08.481631Z","shell.execute_reply":"2024-12-13T14:46:08.522220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## â³ Load and prepare the Data","metadata":{"id":"GN_cpHlSboXV"}},{"cell_type":"code","source":"data = np.load(\"/kaggle/input/homework2/mars_for_students.npz\")\n\ntraining_set = data[\"training_set\"]\nX_train = training_set[:, 0]\ny_train = training_set[:, 1]\n\nX_test = data[\"test_set\"]\n\nprint(f\"Training X shape: {X_train.shape}\")\nprint(f\"Training y shape: {y_train.shape}\")\nprint(f\"Test X shape: {X_test.shape}\")","metadata":{"id":"pLaoDaG1V1Yg","outputId":"94b2ba51-e0a4-4b54-a8f3-b9d57cfde66c","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:45:10.449990Z","iopub.execute_input":"2024-12-13T14:45:10.450731Z","iopub.status.idle":"2024-12-13T14:45:11.800910Z","shell.execute_reply.started":"2024-12-13T14:45:10.450694Z","shell.execute_reply":"2024-12-13T14:45:11.800092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add color channel and rescale pixels between 0 and 1\nX_train = X_train[..., np.newaxis] / 255.0\nX_test = X_test[..., np.newaxis] / 255.0\n\ninput_shape = X_train.shape[1:]\nnum_classes = len(np.unique(y_train))\n\nprint(f\"Input shape: {input_shape}\")\nprint(f\"Number of classes: {num_classes}\")","metadata":{"id":"VmnTgJi_OOs1","outputId":"18634c98-4b98-4501-d71a-316d1067069a","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:45:12.329037Z","iopub.execute_input":"2024-12-13T14:45:12.329939Z","iopub.status.idle":"2024-12-13T14:45:13.221820Z","shell.execute_reply.started":"2024-12-13T14:45:12.329902Z","shell.execute_reply":"2024-12-13T14:45:13.220801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove outliers (containing aliens)\noutliers = [62, 79, 125, 139, 142, 147, 152, 156, 170, 210, 217, 266, 289, 299, 313, 339, 348, 365, 412, 417, 426, 450, 461, 536, 552, 669, 675, 741, 744, 747, 799, 802, 808, 820, 821, 849, 863, 890, 909, 942, 971, 1005, 1057, 1079, 1082, 1092, 1095, 1106, 1119, 1125, 1177, 1194, 1224, 1247, 1248, 1258, 1261, 1262, 1306, 1324, 1365, 1370, 1443, 1449, 1508, 1509, 1519, 1551, 1584, 1588, 1628, 1637, 1693, 1736, 1767, 1768, 1782, 1813, 1816, 1834, 1889, 1925, 1942, 1975, 1979, 2000, 2002, 2086, 2096, 2110, 2111, 2151, 2161, 2222, 2235, 2239, 2242, 2301, 2307, 2350, 2361, 2365, 2372, 2414, 2453, 2522, 2535, 2561, 2609, 2614]\nX_train = np.delete(X_train, outliers, axis=0)\ny_train = np.delete(y_train, outliers, axis=0)\n\n# Print the new shape\nprint(f\"Training X shape after outlier removal: {X_train.shape}\")\nprint(f\"Training y shape after outlier removal: {y_train.shape}\")\nprint(f\"Test X shape: {X_test.shape}\")","metadata":{"id":"T0YMlH-z4XY3","outputId":"3211653a-cac4-4179-d351-f2b029a4516b","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:45:17.821020Z","iopub.execute_input":"2024-12-13T14:45:17.821407Z","iopub.status.idle":"2024-12-13T14:45:17.925140Z","shell.execute_reply.started":"2024-12-13T14:45:17.821376Z","shell.execute_reply":"2024-12-13T14:45:17.924263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of pixels with a given label\nlabel_list, counts = np.unique(y_train, return_counts=True)\nfor label, count in zip(label_list, counts):\n    print(f\"Number of pixels with label {int(label)}: {count}\")","metadata":{"id":"yOybrVCSWbSM","outputId":"27922b4f-8971-4671-d6b1-3e457bc41b3f","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:45:19.711440Z","iopub.execute_input":"2024-12-13T14:45:19.712324Z","iopub.status.idle":"2024-12-13T14:45:20.787318Z","shell.execute_reply.started":"2024-12-13T14:45:19.712259Z","shell.execute_reply":"2024-12-13T14:45:20.786353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_weights = [sum(counts) / counts[i] for i in range(num_classes)]\nclass_weights[0] = 0\nprint(class_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:50:07.284999Z","iopub.execute_input":"2024-12-13T14:50:07.285383Z","iopub.status.idle":"2024-12-13T14:50:07.290834Z","shell.execute_reply.started":"2024-12-13T14:50:07.285351Z","shell.execute_reply":"2024-12-13T14:50:07.289948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the number of images containing a pixel with a given label\ncounts = [0 for _ in range(num_classes)]\nfor labels in y_train:\n    label_list = np.unique(labels)\n    for value in label_list:\n        counts[int(value)] += 1\nfor label, count in enumerate(counts):\n    print(f\"Number of images containing label {int(label)}: {count}\")","metadata":{"id":"JZnHdPUyWbSM","outputId":"7fd61944-9265-49e5-aa82-54243597ffe7","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.319116Z","iopub.execute_input":"2024-12-13T14:21:30.319395Z","iopub.status.idle":"2024-12-13T14:21:30.473810Z","shell.execute_reply.started":"2024-12-13T14:21:30.319369Z","shell.execute_reply":"2024-12-13T14:21:30.472794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Since there are few images with label 4, stratify using the presence of label 4\nstratify=[]\nfor labels in y_train:\n    label_list = np.unique(labels)\n    if 4.0 in label_list:\n        stratify.append(1)\n    else:\n        stratify.append(0)\nstratify = np.array(stratify)","metadata":{"id":"Wjd3qq9eWbSM","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.475155Z","iopub.execute_input":"2024-12-13T14:21:30.475430Z","iopub.status.idle":"2024-12-13T14:21:30.631291Z","shell.execute_reply.started":"2024-12-13T14:21:30.475403Z","shell.execute_reply":"2024-12-13T14:21:30.630335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into training and validation (80-20)\nval_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, random_state=seed, test_size=val_size, stratify=stratify\n)","metadata":{"id":"i-n9_2Eftvp5","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.632385Z","iopub.execute_input":"2024-12-13T14:21:30.632643Z","iopub.status.idle":"2024-12-13T14:21:30.733955Z","shell.execute_reply.started":"2024-12-13T14:21:30.632619Z","shell.execute_reply":"2024-12-13T14:21:30.733268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check imbalance between training and validation\ntrain_counts = [0 for _ in range(num_classes)]\nfor labels in y_train:\n    label_list = np.unique(labels)\n    for value in label_list:\n        train_counts[int(value)] += 1\n\nval_counts = [0 for _ in range(num_classes)]\nfor labels in y_val:\n    label_list = np.unique(labels)\n    for value in label_list:\n        val_counts[int(value)] += 1\n\nratios = [val_counts[i] / train_counts[i] / val_size for i in range(num_classes)]\n\nfor i in range(num_classes):\n    print(f\"Ratio of images containing a label in validation vs train compared to ideal split for label {i}: {ratios[i]}\")","metadata":{"id":"Pm7vk2q3WbSN","outputId":"f507b092-309a-49ce-80d0-cdf8f4ad841d","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.735019Z","iopub.execute_input":"2024-12-13T14:21:30.735418Z","iopub.status.idle":"2024-12-13T14:21:30.891348Z","shell.execute_reply.started":"2024-12-13T14:21:30.735379Z","shell.execute_reply":"2024-12-13T14:21:30.890468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"category_map = {\n    0: 0,  # background\n    1: 1,  # soil\n    2: 2,  # bedrock\n    3: 3,  # sand\n    4: 4,  # big rock\n}\n\n\ndef apply_category_mapping(label):\n    \"\"\"\n    Apply category mapping to labels.\n    \"\"\"\n    keys_tensor = tf.constant(list(category_map.keys()), dtype=tf.int32)\n    vals_tensor = tf.constant(list(category_map.values()), dtype=tf.int32)\n    table = tf.lookup.StaticHashTable(\n        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=0\n    )\n    return table.lookup(label)","metadata":{"id":"bScns-4GYaOt","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.892499Z","iopub.execute_input":"2024-12-13T14:21:30.892793Z","iopub.status.idle":"2024-12-13T14:21:30.898268Z","shell.execute_reply.started":"2024-12-13T14:21:30.892767Z","shell.execute_reply":"2024-12-13T14:21:30.897286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef random_flip_h(image, label):\n    \"\"\"Consistent random horizontal flip.\"\"\"\n    flip_prob = tf.random.uniform([])\n    image = tf.cond(\n        flip_prob > 0.5, lambda: tf.image.flip_left_right(image), lambda: image\n    )\n    label = tf.cond(\n        flip_prob > 0.5, lambda: tf.image.flip_left_right(label), lambda: label\n    )\n    return image, label\n\n@tf.function\ndef random_flip_v(image, label):\n    \"\"\"Consistent random vertical flip.\"\"\"\n    flip_prob = tf.random.uniform([])\n    image = tf.cond(\n        flip_prob > 0.5, lambda: tf.image.flip_up_down(image), lambda: image\n    )\n    label = tf.cond(\n        flip_prob > 0.5, lambda: tf.image.flip_up_down(label), lambda: label\n    )\n    return image, label\n\n@tf.function\ndef random_brightness(image):\n    delta = tf.random.uniform([])\n    delta_thresh = 0.1\n    image = tf.cond(\n        delta < 3*delta_thresh, lambda: tf.image.adjust_brightness(image, delta=delta), lambda: image\n    )\n    return image\n\n@tf.function\ndef random_contrast(image):\n    factor = tf.random.uniform([])\n    factor_thresh = 2\n    image = tf.cond(\n        factor < factor_thresh/6, lambda: tf.image.adjust_contrast(image, contrast_factor=factor), lambda: image\n    )\n    return image\n\n@tf.function\ndef augmentation(image, label, seed=None):\n    image, label = random_flip_h(image, label)\n    #image, label = random_flip_v(image, label)\n    #image = random_brightness(image)\n    #image = random_contrast(image)\n    return image, label","metadata":{"id":"rTVu93r6Yb8r","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.903318Z","iopub.execute_input":"2024-12-13T14:21:30.903645Z","iopub.status.idle":"2024-12-13T14:21:30.914196Z","shell.execute_reply.started":"2024-12-13T14:21:30.903620Z","shell.execute_reply":"2024-12-13T14:21:30.913458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_dataset(images, labels, batch_size, shuffle=True, augment=False, seed=None):\n    \"\"\"\n    Create a memory-efficient TensorFlow dataset.\n    \"\"\"\n    # Add an axis to labels\n    new_labels = labels[..., np.newaxis]\n\n    # Create dataset from file paths\n    dataset = tf.data.Dataset.from_tensor_slices((images, new_labels))\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=batch_size * 2, seed=seed)\n\n    # Apply category mapping\n    dataset = dataset.map(\n        lambda x, y: (x, apply_category_mapping(y)), num_parallel_calls=tf.data.AUTOTUNE\n    )\n\n    if augment:\n        dataset = dataset.map(\n            lambda x, y: augmentation(x, y, seed=seed),\n            num_parallel_calls=tf.data.AUTOTUNE,\n        )\n\n    # Batch the data\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n    return dataset","metadata":{"id":"-kKiHcV0YePz","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.915113Z","iopub.execute_input":"2024-12-13T14:21:30.915424Z","iopub.status.idle":"2024-12-13T14:21:30.930199Z","shell.execute_reply.started":"2024-12-13T14:21:30.915399Z","shell.execute_reply":"2024-12-13T14:21:30.929392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16\n\n# Create the datasets\nprint(\"Creating datasets...\")\ntrain_dataset = make_dataset(\n    X_train,\n    y_train.astype(\"int32\"),\n    batch_size=batch_size,\n    shuffle=False,\n    augment=False,\n    seed=seed,\n)\n\nval_dataset = make_dataset(\n    X_val, y_val.astype(\"int32\"), batch_size=batch_size, shuffle=False\n)\n\nprint(\"Datasets created!\")\n\n# Check the shape of the data\nfor images, labels in train_dataset.take(1):\n    input_shape = images.shape[1:]\n    print(f\"\\nInput shape: {input_shape}\")\n    print(\"Images shape:\", images.shape)\n    print(\"Labels shape:\", labels.shape)\n    print(\"Labels dtype:\", labels.dtype)\n    break","metadata":{"id":"QWAcW_6IY2D0","outputId":"b25677ad-7616-4747-ba61-5591aac27d38","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:30.931100Z","iopub.execute_input":"2024-12-13T14:21:30.931333Z","iopub.status.idle":"2024-12-13T14:21:33.297357Z","shell.execute_reply.started":"2024-12-13T14:21:30.931310Z","shell.execute_reply":"2024-12-13T14:21:33.296498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyze the data","metadata":{"id":"X-oP7zvmjmfx"}},{"cell_type":"code","source":"def create_segmentation_colormap(num_classes):\n    \"\"\"\n    Create a linear colormap using a predefined palette.\n    Uses 'viridis' as default because it is perceptually uniform\n    and works well for colorblindness.\n    \"\"\"\n    return plt.cm.viridis(np.linspace(0, 1, num_classes))\n\n\ndef apply_colormap(label, colormap=None):\n    \"\"\"\n    Apply the colormap to a label.\n    \"\"\"\n    # Ensure label is 2D\n    label = np.squeeze(label)\n\n    if colormap is None:\n        num_classes = len(np.unique(label))\n        colormap = create_segmentation_colormap(num_classes)\n\n    # Apply the colormap\n    colored = colormap[label.astype(int)]\n\n    return colored\n\n\ndef plot_sample_batch(images, labels, num_samples=3):\n    \"\"\"\n    Display some image and label pairs from the dataset.\n    \"\"\"\n    plt.figure(figsize=(15, 4 * num_samples))\n\n    colormap = create_segmentation_colormap(num_classes)\n\n    for j in range(min(num_samples, len(images))):\n        # Plot original image\n        plt.subplot(num_samples, 2, j * 2 + 1)\n        plt.imshow(images[j], cmap=\"gray\")\n        plt.title(f\"Image {j+1}\")\n        plt.axis(\"off\")\n\n        # Plot colored label\n        plt.subplot(num_samples, 2, j * 2 + 2)\n        colored_label = apply_colormap(labels[j], colormap)\n        plt.imshow(colored_label)\n        plt.title(f\"Label {j+1}\")\n        plt.axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n    plt.close()\n\n\n# Visualize examples from the training set\nprint(\"Visualizing examples from the training set:\")\nplot_sample_batch(X_train, y_train, num_samples=10)","metadata":{"id":"J32MOnQaq_GA","outputId":"a8c4eda8-4a11-4d57-97c9-b6cacd92a1b4","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:33.298621Z","iopub.execute_input":"2024-12-13T14:21:33.299294Z","iopub.status.idle":"2024-12-13T14:21:35.336469Z","shell.execute_reply.started":"2024-12-13T14:21:33.299244Z","shell.execute_reply":"2024-12-13T14:21:35.335509Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize examples from the test set.\nnum_samples = 4\nplt.figure(figsize=(15, 2 * num_samples))\n\ncolormap = create_segmentation_colormap(num_classes)\n\nfor j in range(min(num_samples, len(X_test))):\n    plt.subplot(num_samples, 1, j + 1)\n    plt.imshow(X_test[j], cmap=\"gray\")\n    plt.title(f\"Image {j}\")\n    plt.axis(\"off\")","metadata":{"id":"xgIE-cdNLn4s","outputId":"927c7b23-a229-4a67-aae8-95e46b69e54b","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.338401Z","iopub.execute_input":"2024-12-13T14:21:35.338759Z","iopub.status.idle":"2024-12-13T14:21:35.699118Z","shell.execute_reply.started":"2024-12-13T14:21:35.338722Z","shell.execute_reply":"2024-12-13T14:21:35.698249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ› ï¸ Define the model","metadata":{"id":"FSliIxBvbs2Q"}},{"cell_type":"code","source":"def unet_block(\n    input_tensor, filters, kernel_size=3, activation=\"relu\", stack=1, name=\"\", groups=8, dilation_rate=1\n):\n    # Initialise the input tensor\n    x = input_tensor\n\n    # Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n    for i in range(stack):\n        x = tfkl.Conv2D(\n            filters,\n            kernel_size=kernel_size,\n            padding=\"same\",\n            dilation_rate=dilation_rate,\n            kernel_regularizer=tfk.regularizers.L2(1e-3),\n            name=name + \"conv\" + str(i + 1),\n        )(x)\n        x = tfkl.GroupNormalization(name=name + \"bn\" + str(i + 1), groups=groups)(x)\n        x = tfkl.Activation(activation, name=name + \"activation\" + str(i + 1))(x)\n\n    # Return the transformed tensor\n    return x","metadata":{"id":"uLNQR71Lsf7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.700751Z","iopub.execute_input":"2024-12-13T14:21:35.701290Z","iopub.status.idle":"2024-12-13T14:21:35.707925Z","shell.execute_reply.started":"2024-12-13T14:21:35.701239Z","shell.execute_reply":"2024-12-13T14:21:35.707028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a Residual block with configurable parameters (currently unused, but\n# used in older models). Note that a modified version of this block is used in\n# the final model, by combining it with the inception block.\ndef residual_unet_block(x, filters, kernel_size=3, padding='same',\n                        downsample=False, activation='relu', stack=2, name='residual', groups=8, dilation_rate=1):\n\n    for s in range(stack):\n        # Save input for skip connection.\n        skip = x\n\n        # First convolutional block with Batch Normalisation and activation.\n        x = tfkl.Conv2D(filters, kernel_size, padding=padding, name=f'{name}_conv1_{s}', dilation_rate=dilation_rate)(x)\n        x = tfkl.GroupNormalization(name=f'{name}_bn1_{s}', groups=groups)(x)\n        x = tfkl.Activation(activation, name=f'{name}_act1_{s}')(x)\n\n        # Second convolutional block.\n        x = tfkl.Conv2D(filters, kernel_size, padding=padding, name=f'{name}_conv2_{s}', dilation_rate=dilation_rate)(x)\n        x = tfkl.GroupNormalization(name=f'{name}_bn2_{s}', groups=groups)(x)\n\n        # Adjust skip connection dimension if needed.\n        if skip.shape[-1] != filters:\n            skip = tfkl.Conv2D(filters, 1, padding=padding, name=f'{name}_proj_{s}')(skip)\n            skip = tfkl.GroupNormalization(name=f'{name}_proj_bn_{s}', groups=groups)(skip)\n\n        # Add skip connection and apply activation.\n        x = tfkl.Add(name=f'{name}_add_{s}')([x, skip])\n        x = tfkl.Activation(activation, name=f'{name}_act2_{s}')(x)\n\n    # Optional downsampling.\n    if downsample:\n        x = tfkl.MaxPooling2D(2, name=f'{name}_pool')(x)\n\n    return x","metadata":{"id":"SxxhUBMy4XY6","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.708970Z","iopub.execute_input":"2024-12-13T14:21:35.709480Z","iopub.status.idle":"2024-12-13T14:21:35.723936Z","shell.execute_reply.started":"2024-12-13T14:21:35.709452Z","shell.execute_reply":"2024-12-13T14:21:35.723254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the Inception block with batch normalization (BN) and with multiple\n# convolution paths and optional downsampling.\ndef inception_block_bn(x, filters, padding='same',\n                       downsample=False, activation='relu', stack=1, name='inception', groups=8):\n    # This inception block consists of a 1x1 convolution path, a 3x3 convolution\n    # path, a 5x5 convolution path, and a pooling path. The paths are then\n    # concatenated to form the final block output.\n    # The reason for using multiple paths is to allow the model to learn\n    # different features at different scales, and to increase the model's\n    # capacity without increasing the number of parameters too much.\n    # The downsampling (pooling) is not mandatory.\n\n    # Loop through specified stack layers for multiple inception paths.\n    for s in range(stack):\n        # 1x1 convolution path with batch normalization and activation.\n        conv1 = tfkl.Conv2D(filters // 4, 1, padding=padding, name=f'{name}_conv1_{s}')(x)\n        conv1 = tfkl.GroupNormalization(name=f'{name}_bn1_{s}', groups=groups)(conv1)\n        conv1 = tfkl.Activation(activation, name=f'{name}_act1_{s}')(conv1)\n\n        # 3x3 convolution path with initial reduction layer.\n        conv3_reduce = tfkl.Conv2D(filters // 8, 1, padding=padding, name=f'{name}_conv3_reduce_{s}')(x)\n        conv3_reduce = tfkl.GroupNormalization(name=f'{name}_bn3_reduce_{s}', groups=groups)(conv3_reduce)\n        conv3_reduce = tfkl.Activation(activation, name=f'{name}_act3_reduce_{s}')(conv3_reduce)\n        conv3 = tfkl.Conv2D(filters // 4, 3, padding=padding, name=f'{name}_conv3_{s}')(conv3_reduce)\n        conv3 = tfkl.GroupNormalization(name=f'{name}_bn3_{s}', groups=groups)(conv3)\n        conv3 = tfkl.Activation(activation, name=f'{name}_act3_{s}')(conv3)\n\n        # 5x5 convolution path with initial reduction layer.\n        conv5_reduce = tfkl.Conv2D(filters // 12, 1, padding=padding, name=f'{name}_conv5_reduce_{s}')(x)\n        conv5_reduce = tfkl.GroupNormalization(name=f'{name}_bn5_reduce_{s}', groups=groups)(conv5_reduce)\n        conv5_reduce = tfkl.Activation(activation, name=f'{name}_act5_reduce_{s}')(conv5_reduce)\n        conv5 = tfkl.Conv2D(filters // 4, 5, padding=padding, name=f'{name}_conv5_{s}')(conv5_reduce)\n        conv5 = tfkl.GroupNormalization(name=f'{name}_bn5_{s}', groups=groups)(conv5)\n        conv5 = tfkl.Activation(activation, name=f'{name}_act5_{s}')(conv5)\n\n        # Pooling path with projection for spatial dimensionality reduction.\n        pool = tfkl.MaxPooling2D(3, strides=1, padding=padding, name=f'{name}_pooling_{s}')(x)\n        pool_proj = tfkl.Conv2D(filters // 4, 1, padding=padding, name=f'{name}_pool_proj_{s}')(pool)\n        pool_proj = tfkl.GroupNormalization(name=f'{name}_bn_pool_proj_{s}', groups=groups)(pool_proj)\n        pool_proj = tfkl.Activation(activation, name=f'{name}_act_pool_proj_{s}')(pool_proj)\n\n        # Concatenate all paths to form the final block output.\n        x = tfkl.Concatenate(name=f'{name}_concat_{s}')([conv1, conv3, conv5, pool_proj])\n\n    # Apply downsampling if specified.\n    if downsample:\n        x = tfkl.MaxPooling2D(2, name=f'{name}_pool')(x)\n    return x","metadata":{"id":"QSZB44pd4XY7","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.725225Z","iopub.execute_input":"2024-12-13T14:21:35.725668Z","iopub.status.idle":"2024-12-13T14:21:35.741100Z","shell.execute_reply.started":"2024-12-13T14:21:35.725629Z","shell.execute_reply":"2024-12-13T14:21:35.740358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the a residual block using an inception block instead of the\n# convolutional path. This mixed block is used in the final model, and it\n# combines the benefits of both residual and inception blocks. We believe\n# that the residual and inception blocks are complementary, as their guesses\n# don't seem to perfectly overlap. See the report for more details.\ndef inception_residual_unet(x, filters, padding='same', downsample=False,\n                            activation='relu', stack=4, inception_stack=1,\n                            name='residual', groups=8):\n\n    for s in range(stack):\n        # Save input for skip connection.\n        skip = x\n\n        # Create the inception block.\n        x = inception_block_bn(x, filters, padding, downsample=False, activation=activation,\n                               stack=inception_stack, name=f'{name}_inception_{s}', groups=groups)\n\n        # Adjust skip connection dimension if needed.\n        if skip.shape[-1] != filters:\n            skip = tfkl.Conv2D(filters, 1, padding=padding, name=f'{name}_proj_{s}')(skip)\n            skip = tfkl.GroupNormalization(name=f'{name}_proj_bn_{s}', groups=groups)(skip)\n\n        # Add skip connection and apply activation.\n        x = tfkl.Add(name=f'{name}_add_{s}')([x, skip])\n        x = tfkl.Activation(activation, name=f'{name}_act2_{s}')(x)\n\n    # Optional downsampling.\n    if downsample:\n        x = tfkl.MaxPooling2D(2, name=f'{name}_pool')(x)\n\n    return x","metadata":{"id":"OhzrNA4z4XY7","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.742227Z","iopub.execute_input":"2024-12-13T14:21:35.742832Z","iopub.status.idle":"2024-12-13T14:21:35.758317Z","shell.execute_reply.started":"2024-12-13T14:21:35.742778Z","shell.execute_reply":"2024-12-13T14:21:35.757525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def attention_gate(input_tensor, gating_tensor, inter_channels):\n    # 1x1 convolution on the input (skip connection)\n    theta_x = tfkl.Conv2D(inter_channels, kernel_size=1, strides=2, padding='same')(input_tensor)\n    # 1x1 convolution on the gating (decoder output)\n    phi_g = tfkl.Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(gating_tensor)\n    # Add and apply ReLU\n    add = tfkl.Add()([theta_x, phi_g])\n    relu = tfkl.Activation('relu')(add)\n    # Generate attention weights\n    psi = tfkl.Conv2D(1, kernel_size=1, strides=1, padding='same', activation='sigmoid')(relu)\n    # Upsample attention weights to match input_tensor spatial dimensions\n    upsampled_psi = tfkl.UpSampling2D()(psi)\n    # Multiply input tensor by attention weights\n    output = tfkl.Multiply()([input_tensor, upsampled_psi])\n    return output","metadata":{"id":"wAic1TwgWbSP","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.759348Z","iopub.execute_input":"2024-12-13T14:21:35.759694Z","iopub.status.idle":"2024-12-13T14:21:35.772349Z","shell.execute_reply.started":"2024-12-13T14:21:35.759657Z","shell.execute_reply":"2024-12-13T14:21:35.771624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_unet_model(input_shape=(64, 128, 1), num_classes=num_classes, seed=seed):\n    tf.random.set_seed(seed)\n    input_layer = tfkl.Input(shape=input_shape, name=\"input_layer\")\n    base_filters = 32\n\n    # Downsampling path\n    down_block_1 = unet_block(input_layer, base_filters, name=\"down_block1_\", groups=1)\n    d1 = tfkl.SpatialDropout2D(0.05)(down_block_1)\n    d1 = tfkl.MaxPooling2D()(d1)\n\n    down_block_2 = unet_block(d1, base_filters*2, name=\"down_block2_\", groups=2)\n    d2 = tfkl.SpatialDropout2D(0.1)(down_block_2)\n    d2 = tfkl.MaxPooling2D()(d2)\n\n    down_block_3 = unet_block(d2, base_filters*4, name=\"down_block3_\", groups=4)\n    d3 = tfkl.SpatialDropout2D(0.15)(down_block_3)\n    d3 = tfkl.MaxPooling2D()(d3)\n\n    down_block_4 = unet_block(d3, base_filters*8, name=\"down_block4_\", groups=8)\n    d4 = tfkl.SpatialDropout2D(0.2)(down_block_4)\n    d4 = tfkl.MaxPooling2D()(d4)\n\n    down_block_5 = unet_block(d4, base_filters*16, name=\"down_block5_\", groups=8)\n    d5 = tfkl.SpatialDropout2D(0.25)(down_block_5)\n    d5 = tfkl.MaxPooling2D()(d5)\n\n    # Bottleneck\n    bottleneck = unet_block(d5, base_filters*32, name=\"bottleneck\", groups=8, stack=2, dilation_rate=2)\n    bottleneck = tfkl.SpatialDropout2D(0.3)(bottleneck)\n\n    # Upsampling path\n    u1 = tfkl.Conv2DTranspose(base_filters*16, kernel_size=2, strides=2, padding=\"same\")(bottleneck) \n    u1 = tfkl.Concatenate()([u1, down_block_5])\n    u1 = unet_block(u1, base_filters*16, name=\"up_block1_\", groups=8) \n\n    u2 = tfkl.Conv2DTranspose(base_filters*8, kernel_size=2, strides=2, padding=\"same\")(u1) \n    u2 = tfkl.Concatenate()([u2, down_block_4])\n    u2 = unet_block(u2, base_filters*8, name=\"up_block2_\", groups=8)\n\n    u3 = tfkl.Conv2DTranspose(base_filters*4, kernel_size=2, strides=2, padding=\"same\")(u2) \n    u3 = tfkl.Concatenate()([u3, down_block_3])\n    u3 = unet_block(u3, base_filters*4, name=\"up_block3_\", groups=4)\n\n    u4 = tfkl.Conv2DTranspose(base_filters*2, kernel_size=2, strides=2, padding=\"same\")(u3) \n    u4 = tfkl.Concatenate()([u4, down_block_2])\n    u4 = unet_block(u4, base_filters*2, name=\"up_block4_\", groups=2)\n\n    u5 = tfkl.Conv2DTranspose(base_filters, kernel_size=2, strides=2, padding=\"same\")(u4) \n    u5 = tfkl.Concatenate()([u5, down_block_1])\n    u5 = unet_block(u5, base_filters, name=\"up_block5_\", groups=1)\n\n    # Output Layer\n    output_layer = tfkl.Conv2D(\n        num_classes,\n        kernel_size=1,\n        padding=\"same\",\n        activation=\"softmax\",\n        name=\"output_layer\",\n    )(u5)\n\n    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name=\"UNet\")\n    return model","metadata":{"id":"NRNEskCXsiM_","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.773234Z","iopub.execute_input":"2024-12-13T14:21:35.773507Z","iopub.status.idle":"2024-12-13T14:21:35.789551Z","shell.execute_reply.started":"2024-12-13T14:21:35.773463Z","shell.execute_reply":"2024-12-13T14:21:35.788657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define parameters\nepochs = 1000\npatience = 45\nlearning_rate = 1e-4","metadata":{"id":"VP8fEvC9vpsr","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.790908Z","iopub.execute_input":"2024-12-13T14:21:35.791218Z","iopub.status.idle":"2024-12-13T14:21:35.802827Z","shell.execute_reply.started":"2024-12-13T14:21:35.791189Z","shell.execute_reply":"2024-12-13T14:21:35.801963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_unet_model()\n\n# Print a detailed summary of the model with expanded nested layers and trainable parameters.\nmodel.summary(expand_nested=True, show_trainable=True)\n\n# Generate and display a graphical representation of the model architecture.\ntry:\n    tf.keras.utils.plot_model(model, show_trainable=True, expand_nested=True, dpi=70)\nexcept:\n    print(\"Model too complex to plot!\")","metadata":{"id":"CBkb3TRF1KJx","outputId":"95f09d88-43e9-4f59-886a-385bae6e5a63","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:35.803856Z","iopub.execute_input":"2024-12-13T14:21:35.804177Z","iopub.status.idle":"2024-12-13T14:21:36.615501Z","shell.execute_reply.started":"2024-12-13T14:21:35.804147Z","shell.execute_reply":"2024-12-13T14:21:36.614738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train the model","metadata":{"id":"eDBA-uBFsRAd"}},{"cell_type":"code","source":"# Define custom Mean Intersection Over Union metric\n@tfk.utils.register_keras_serializable()\nclass MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n    def __init__(\n        self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None\n    ):\n        super(MeanIntersectionOverUnion, self).__init__(\n            num_classes=num_classes, name=name, dtype=dtype\n        )\n        if labels_to_exclude is None:\n            labels_to_exclude = [0]  # Default to excluding label 0\n        self.labels_to_exclude = labels_to_exclude\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        # Convert predictions to class labels\n        y_pred = tf.math.argmax(y_pred, axis=-1)\n\n        # Flatten the tensors\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.reshape(y_pred, [-1])\n\n        # Apply mask to exclude specified labels\n        for label in self.labels_to_exclude:\n            mask = tf.not_equal(y_true, label)\n            y_true = tf.boolean_mask(y_true, mask)\n            y_pred = tf.boolean_mask(y_pred, mask)\n\n        # Update the state\n        return super().update_state(y_true, y_pred, sample_weight)\n\n\n# Visualization callback\nclass VizCallback(tf.keras.callbacks.Callback):\n    def __init__(self, image, label, frequency=5):\n        super().__init__()\n        self.image = image\n        self.label = label\n        self.frequency = frequency\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % self.frequency == 0:  # Visualize only every \"frequency\" epochs\n            image, label = self.image, self.label\n            label = apply_category_mapping(label)\n            pred = self.model.predict(image, verbose=0)\n            y_pred = tf.math.argmax(pred, axis=-1)\n            y_pred = y_pred.numpy()\n\n            # Create colormap\n            colormap = create_segmentation_colormap(num_classes)\n\n            plt.figure(figsize=(16, 4))\n\n            # Input image\n            plt.subplot(1, 3, 1)\n            plt.imshow(image[0], cmap=\"gray\")\n            plt.title(\"Input Image\")\n            plt.axis(\"off\")\n\n            # Ground truth\n            plt.subplot(1, 3, 2)\n            colored_label = apply_colormap(label.numpy(), colormap)\n            plt.imshow(colored_label)\n            plt.title(\"Ground Truth Mask\")\n            plt.axis(\"off\")\n\n            # Prediction\n            plt.subplot(1, 3, 3)\n            colored_pred = apply_colormap(y_pred[0], colormap)\n            plt.imshow(colored_pred)\n            plt.title(\"Predicted Mask\")\n            plt.axis(\"off\")\n\n            plt.tight_layout()\n            plt.show()\n            plt.close()","metadata":{"id":"GyHMGG5EdXGr","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.616870Z","iopub.execute_input":"2024-12-13T14:21:36.617207Z","iopub.status.idle":"2024-12-13T14:21:36.628151Z","shell.execute_reply.started":"2024-12-13T14:21:36.617176Z","shell.execute_reply":"2024-12-13T14:21:36.627264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef iou_loss(y_true, y_pred, num_classes=num_classes, smooth=1e-6):\n    \"\"\"\n    Compute the Intersection over Union (IoU) loss.\n    :param y_true: Ground truth tensor (not one-hot encoded).\n    :param y_pred: Predicted tensor (probabilities or logits).\n    :param smooth: Smoothing factor to avoid division by zero.\n    :return: IoU loss value.\n    \"\"\"\n    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), num_classes)\n\n    # Initialize a list to store IoU values for each class (ignoring class 0)\n    iou_values = []\n\n    # Loop over all classes (excluding class 0)\n    for i in range(1, num_classes):  # Start from 1 to exclude background (class 0)\n        # Get the probabilities for the current class (class i)\n        y_pred_class = y_pred[..., i]\n        y_true_class = y_true_one_hot[..., i]\n\n        # Flatten the predicted probabilities and true labels for class i\n        y_true_f_class = K.flatten(y_true_class)\n        y_pred_f_class = K.flatten(y_pred_class)\n\n        # Calculate intersection and union for class i\n        intersection = K.sum(y_true_f_class * y_pred_f_class)\n        union = K.sum(y_true_f_class) + K.sum(y_pred_f_class) - intersection\n\n        # Compute IoU for this class\n        iou_class = (intersection + smooth) / (union + smooth)\n        iou_values.append(iou_class)\n\n    # Compute the mean IoU over all classes (excluding class 0)\n    mean_iou = K.mean(K.stack(iou_values))\n\n    # Return mean IoU loss (1 - mean IoU)\n    return 1 - mean_iou","metadata":{"id":"zOOL9nbyXzuJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.629198Z","iopub.execute_input":"2024-12-13T14:21:36.629520Z","iopub.status.idle":"2024-12-13T14:21:36.643685Z","shell.execute_reply.started":"2024-12-13T14:21:36.629487Z","shell.execute_reply":"2024-12-13T14:21:36.642904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_loss(y_true, y_pred, num_classes=num_classes, ignore_class=0, smooth = 1e-6):\n    \"\"\"\n    Computes the Dice loss with the option to ignore a specific class.\n\n    Parameters:\n        y_true (tensor): Ground truth segmentation mask (one-hot encoded).\n        y_pred (tensor): Predicted segmentation mask (one-hot encoded).\n        ignore_class (int): The class label to be ignored in the calculation (default is 0).\n\n    Returns:\n        float: The Dice loss, excluding the `ignore_class`.\n    \"\"\"\n    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), num_classes)\n\n    # Flatten the tensors to compute dice coefficient for each class\n    y_true_f = K.flatten(y_true_one_hot)\n    y_pred_f = K.flatten(y_pred)\n\n    # Calculate the Dice coefficient for each class\n    dice_per_class = []\n\n    for i in range(num_classes):\n        if i == ignore_class:\n            continue\n        y_pred_class = y_pred[..., i]\n        y_true_class = y_true_one_hot[..., i]\n\n        y_true_f_class = K.flatten(y_true_class)\n        y_pred_f_class = K.flatten(y_pred_class)\n\n        intersection = K.sum(y_true_f_class * y_pred_f_class)\n        union = K.sum(y_true_f_class) + K.sum(y_pred_f_class) - intersection\n\n        dice_coeff = (2. * intersection + smooth) / (union + smooth)\n        dice_per_class.append(dice_coeff)\n\n    # Return the mean Dice coefficient\n    return 1 - K.mean(K.stack(dice_per_class))","metadata":{"id":"-JV4PB3Lbd42","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.644621Z","iopub.execute_input":"2024-12-13T14:21:36.644865Z","iopub.status.idle":"2024-12-13T14:21:36.658525Z","shell.execute_reply.started":"2024-12-13T14:21:36.644840Z","shell.execute_reply":"2024-12-13T14:21:36.657858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a custom loss\nclass CustomLoss(tfk.losses.Loss):\n    def __init__(self, alpha, num_classes=num_classes, name='custom_loss', **kwargs):\n        super(CustomLoss, self).__init__(name=name, **kwargs)\n        self.alpha = alpha\n        self.num_classes = num_classes\n\n    def call(self, y_true, y_pred):\n        # Calculate the loss\n        return (1-self.alpha)*focal_loss.sparse_categorical_focal_loss(y_true, y_pred, class_weight=class_weights, gamma=3.0)+self.alpha*iou_loss(y_true, y_pred)","metadata":{"id":"T4Fw7pgu4XY8","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.659486Z","iopub.execute_input":"2024-12-13T14:21:36.659723Z","iopub.status.idle":"2024-12-13T14:21:36.672790Z","shell.execute_reply.started":"2024-12-13T14:21:36.659699Z","shell.execute_reply":"2024-12-13T14:21:36.671960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compile the model\nprint(\"Compiling model...\")\nmodel.compile(\n    loss=CustomLoss(0.8),\n    optimizer=tfk.optimizers.AdamW(learning_rate),\n    metrics=[\n        \"accuracy\",\n        MeanIntersectionOverUnion(num_classes=num_classes, labels_to_exclude=[0]),\n    ],\n)\nprint(\"Model compiled!\")","metadata":{"id":"BKIjlRxIdl6z","outputId":"9f80157b-ba5e-4702-8746-937e98640deb","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.673856Z","iopub.execute_input":"2024-12-13T14:21:36.674150Z","iopub.status.idle":"2024-12-13T14:21:36.698855Z","shell.execute_reply.started":"2024-12-13T14:21:36.674105Z","shell.execute_reply":"2024-12-13T14:21:36.697997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup callbacks\nearly_stopping = tfk.callbacks.EarlyStopping(\n    monitor=\"val_mean_iou\", mode=\"max\", patience=patience, restore_best_weights=True\n)\n\nimage, label = val_dataset.take(1).get_single_element()\nviz_callback = VizCallback(image[1:2, ...], label[1:2, ...])\n\nreduce_lr_callback = tfk.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", patience=patience / 3, factor=0.1, min_lr=learning_rate * 1e-4\n)\n\ncallbacks = [early_stopping, viz_callback, reduce_lr_callback]","metadata":{"id":"J2f20uQ5tIMC","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.699908Z","iopub.execute_input":"2024-12-13T14:21:36.700181Z","iopub.status.idle":"2024-12-13T14:21:36.717937Z","shell.execute_reply.started":"2024-12-13T14:21:36.700155Z","shell.execute_reply":"2024-12-13T14:21:36.717330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    epochs=epochs,\n    validation_data=val_dataset,\n    callbacks=callbacks,\n    verbose=1,\n).history\n\n# Calculate and print the final validation accuracy\nfinal_val_meanIoU = round(max(history[\"val_mean_iou\"]) * 100, 2)\nprint(f\"Final validation Mean Intersection Over Union: {final_val_meanIoU}%\")","metadata":{"id":"pMCbSMQ_-XoH","outputId":"7fdb22c5-c431-4d1b-e303-879670c5ee57","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T14:21:36.718798Z","iopub.execute_input":"2024-12-13T14:21:36.719049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"timestep_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\nmodel_filename = f\"model_{timestep_str}.keras\"\nmodel.save(model_filename)\ndel model\n\nprint(f\"Model saved to {model_filename}\")","metadata":{"id":"PtM0ubgdOzG-","outputId":"be5d6595-e892-4488-c071-43bc860cd0e9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ðŸ“Š Prepare Your Submission\n\nIn our Kaggle competition, submissions are made as `csv` files. To create a proper `csv` file, you need to flatten your predictions and include an `id` column as the first column of your dataframe. To maintain consistency between your results and our solution, please avoid shuffling the test set. The code below demonstrates how to prepare the `csv` file from your model predictions.\n\n\n","metadata":{"id":"RNp6pUZuddqC"}},{"cell_type":"code","source":"model = tfk.models.load_model(model_filename, compile=False)\nprint(f\"Model loaded from {model_filename}\")","metadata":{"id":"FMIq69eWgRmr","outputId":"ef114b55-3581-4a47-b4ae-6ae8e1f4f87b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = model.predict(X_test)\npreds = np.argmax(preds, axis=-1)\nprint(f\"Predictions shape: {preds.shape}\")","metadata":{"id":"z287uIQ_VGoK","outputId":"93bf5ed1-d957-4a22-d4c2-6f82ea2497b1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def y_to_df(y) -> pd.DataFrame:\n    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n    n_samples = len(y)\n    y_flat = y.reshape(n_samples, -1)\n    df = pd.DataFrame(y_flat)\n    df[\"id\"] = np.arange(n_samples)\n    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n    return df[cols]","metadata":{"id":"SPjMEKqZW5jX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create and download the csv submission file\ntimestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\nsubmission_filename = f\"submission_{timestep_str}.csv\"\nsubmission_df = y_to_df(preds)\nsubmission_df.to_csv(submission_filename, index=False)\n\n#from google.colab import files\n#files.download(submission_filename)","metadata":{"id":"s18kX1uDconq","outputId":"3e902b7a-a69d-46bc-e066-1cfc931e915f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\nimport seaborn as sns\n\n# Predict class probabilities and get predicted classes on the validation set.\ntest_predictions = np.argmax(model.predict(X_val), axis=-1).flatten()\n\n# Extract ground truth classes.\ntest_gt = y_val.flatten()\n\n# Calculate and display test set accuracy.\ntest_accuracy = accuracy_score(test_gt, test_predictions)\nprint(f'Accuracy score over the test set: {round(test_accuracy, 4)}')\n\n# Calculate and display test set precision.\ntest_precision = precision_score(test_gt, test_predictions, average='weighted')\nprint(f'Precision score over the test set: {round(test_precision, 4)}')\n\n# Calculate and display test set recall.\ntest_recall = recall_score(test_gt, test_predictions, average='weighted')\nprint(f'Recall score over the test set: {round(test_recall, 4)}')\n\n# Calculate and display test set F1 score.\ntest_f1 = f1_score(test_gt, test_predictions, average='weighted')\nprint(f'F1 score over the test set: {round(test_f1, 4)}')\n\n# Compute the confusion matrix.\ncm = confusion_matrix(test_gt, test_predictions)\n\n# Create labels combining confusion matrix values.\nlabels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n\n# Plot the confusion matrix with class labels.\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=labels, fmt='', xticklabels=range(num_classes), yticklabels=range(num_classes), cmap='Blues')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()","metadata":{"id":"lLRqxuiyYXuE","outputId":"8f40a2be-e15a-4cf9-912a-341af3fab40d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  \n<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"15\"> **Instagram:** https://www.instagram.com/airlab_polimi/\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"15\"> **LinkedIn:** https://www.linkedin.com/company/airlab-polimi/\n___\nCredits: Alberto Archetti ðŸ“§ alberto.archetti@polito.it\n\n\n\n\n\n```\n   Copyright 2024 Alberto Archetti\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n```","metadata":{"id":"cQEgmFTPfz1n"}}]}