\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}
\usepackage{enumitem}

\title{AN2DL Reports Template}

\begin{document}

\begin{figure}[H]
      \raggedright
      \includegraphics[scale=0.4]{polimi.png} \hfill
      \includegraphics[scale=0.3]{airlab.jpeg}
\end{figure}

\vspace{5mm}

\begin{center}
      % Select between First and Second
      {\Large \textbf{AN2DL - Second Homework Report}}\\
      \vspace{2mm}
      % Change with your Team Name
      {\Large \textbf{LosPollosHermanos}}\\
      \vspace{2mm}
      % Team Members Information
      {\large Mohammadhossein Allahakbari,}
      {\large Michele Miotti,}
      {\large Francesco Pesce}\\
      \vspace{2mm}
      % Codabench Nicknames
      {mh2033,}
      {michelem,}
      {francescopesce}\\
      \vspace{2mm}
      % Matriculation Numbers
      {246639,}
      {249499,}
      {247974}\\
      \vspace{5mm}
      \today
\end{center}
\vspace{5mm}

\begin{multicols}{2}
      % Note: The following sections represent a suggested
      % structure. We don't need to follow it strictly.

      % -----------------------------------------------------------------------
      % INTRODUCTION
      % -----------------------------------------------------------------------
      \section{Introduction}
      % In this section, you should present your project's context and
      % objectives. You might want to:
      % \begin{itemize}
      %     \item Dehe problem (\textit{you may use italics to highlight
      %               definitions})
      %     \item State your goals (\textbf{emphasise key points with bold})
      %     \item Outline your approach
      % \end{itemize}

      % \noindent For instance, you might write: ``This project focuses on
      % \textit{image classification} using \textbf{deep learning} techniques."

      This report presents the results of the \textit{semantic segmentation}\cite{long2015fullyconvolutionalnetworkssemantic}
      task proposed in the second homework of the \texttt{Artificial Neural Networks and Deep Learning} course. The goal of the project is to classify each pixel in a $64\times128$ grayscale image in one of $5$ classes, each representing a different type of terrain or object on the Mars surface.

      % -----------------------------------------------------------------------
      % PROBLEM ANALYSIS
      % -----------------------------------------------------------------------
      \section{Problem Analysis}
      \label{sec:analysis}
      % Here you can discuss your initial analysis of the problem. Consider
      % including:
      % \begin{enumerate}
      %     % 8 classes, 96x96 rgb images, labels, etc
      %     \item Dataset characteristics
      %     \item Main challenges % The test set is horrible
      %     That the test set was not horrible? Is this what they mean?
      %     \item Initial assumptions 
      % \end{enumerate}

      % \noindent If you need to reference papers, use the citation command:
      % Recent work~\cite{lecun2015deep} suggests..."

      The dataset is composed of $64\times128$ grayscale images, and each pixel of the images belongs to one of five classes (background, soil, bedrock, sand, big rock), resembling a Mars surface classification task\cite{li2024marssegmarssurfacesemantic}. The model is evaluated on the \textit{mean intersection over union (IoU)} metric. After each pixel of all images has been classified, the IoU is evaluated as the ratio of the true positives, as the numerator, and the sum of all positives and false negatives, as the denorminator. The mean IoU is then computed as the average of the IoUs of all non-background classes.

      We were provided two separate datasets. The first dataset contained around $2500$ labeled images, and was used to train the model. The second dataset contained unlabeled images, and was instead used to evaluate our models, by uploading the models' predictions to the \texttt{Kaggle}\cite{kaggle} platform. This last dataset was further split into two subsets, the first of which containing about $25\%$ of the data, used to provide public mean IoU results during the competition, while the other subset contained the rest of the data, and was used for the final evaluation. The three datasets will be reffered to as \texttt{DS1}, \texttt{DS2} and \texttt{DS3} respectively. After manually analyzing \texttt{DS2}, we found that it contained $110$ images which were doctored with unraled overlays, and discarded said images from the training set. Moreover, manually analyzing the labels for \texttt{DS1}, we found them to be rather imprecise, as they were labelled by hand. This lack of precision can be modelled as a contribution of a \textit{bias} of the labelling process, caused by the same type of imprecisions being repeated in a consistent way, and of an innate \textit{variability} of the dataset. In principle, assuming the imprecisions follow the same distribution in \texttt{DS2} and \texttt{DS3}, this should not prove to be an issue, as any bias can be learned by the model, but the added innate variability can never be removed by any model. Nevertheless, relabiling the images could improve the training process, but the idea was discarded as unfeasible.

      % -----------------------------------------------------------------------   
      % METHOD
      % -----------------------------------------------------------------------
      \section{Method}
      % Not sure what they are asking here. The final model?
      % This section should detail your approach. You can use equations to
      % explain your methodology. For example, a simple model representation:
      % \begin{equation}
      %     \label{eq:model}
      %     f(x) = \text{softmax}(Wx + b)
      % \end{equation}

      % \noindent Or a more complex loss function:
      % \begin{equation}
      %     \label{eq:loss}
      %     \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} y_i\log(\hat{y}_i)
      % \end{equation}

      % \noindent Reference these equations in your text, like:``As shown in
      % equation~\ref{eq:model}..."

      Most of the development process was done on \texttt{Jupyter} notebooks, using the \texttt{Tensorflow}\cite{TensorFlow} and \texttt{Keras}\cite{chollet2015keras} libraries for \texttt{Python}. Except for very early models, which were trained on \texttt{Google Colab}, nad the last few models, which were trained on \texttt{Kaggle} due to hardware limitations, training was performed locally using an \texttt{NVIDIA RTX 2060 Mobile}.

      In order to validate the model, we split the training set into a training and validation set, using a $80$ to $20$ ratio in most models, and a $90$ to $10$ ratio in later ones. Differently from the first homework, we noticed that the validation and \texttt{DS2} performances were rather similar, so we were able to avoid submitting models with low performance on the validation set.

      We performed many experiments, with all models sharing the same basic structure, heavily inspired by the U-Net\cite{ronneberger2015unetconvolutionalnetworksbiomedical} architecture, consisting of a contracting path on the left and an expanding path on the right. The contracting path is composed of a series of convolutional and downsampling layers, where each layer halves the spatial dimensions of the input, while doubling the number of channels. The expanding path is composed of a series of convolutional and up-sampling layers, which act as the opposite of the previous layers, doubling the spatial dimensions and halving the number of channels. The final model was trained using the \texttt{Adam} optimizer with an initial learning rate $1e-4$, adaptively lowered after $15$ epoch of stagnation in validation loss, and has $10$ million parameters, achieving a Mean IoU of $0.62$ on \texttt{DS2}.
      %TODO: fix the number of parameters and iou and give more info.
      %TODO: insert the image here.

      % -----------------------------------------------------------------------
      % EXPERIMENTS
      % -----------------------------------------------------------------------
      \section{Succesful experiments}
      \label{sec:succ_experiments}
      In chronological order, the main breakthroughs were:
      \begin{itemize}[leftmargin=*]
            \setlength\itemsep{0em}
            \item \textbf{Trivial U-Net:} We initially started with a simple U-Net architecture, to get a baseline of the model's performance and to understand the reponse of the model to the dataset, and the website's evaluation.
            \item \textbf{Simple augmentation:} Similarly to the first homework, we used simple augmentations such as rotations, flips, and zooms to increase the model's robustness. Of course, this was also applied to the labels, so that they'd refer to the transformed version of their own pixel. We tested more complex augmentations as well, but ultimately noticed no improvement, so augmentations we removed from later models.
            \item \textbf{Lowering the batch size:} Our first major obstacle was the model's inability to learn consistently and predictably. Minor changes to the model or even the seed would result in the training processes halting at a mean IoU of less then $0.1$. The issue was likely the fact that the training process remained stuck in a \textit{local minimum}, and the more \textit{noisy graident estimations} produced by a lower \texttt{batch size} were able to solve the problem. Although this didn't increase the model's performance on its own, it allowed the model to behave more consistently, with a lower result variance, setting the stage for future improvements, and allowing us to use larger networks.
            \item \textbf{Custom block:} By following our previous experience with the first homework, we added our own custom block (described in detail in the previous report), which is formed by a stack of \texttt{Inception} blocks followed by residual connections, in place of the convolutional layers.
            \item \textbf{IoU loss function:} To directly optimize the model for the evaluation, we decided to implement a loss function directily based on the metric to optimize, by directly accessing the backend from \texttt{Keras}. The loss function differs from the mean IoU metric in two things: since it needs to be \textit{differentiable}, it is computed using the posterior probabilities for classes outputted by the model instead of just the maximum of said probabilities, and since it needs to have a minimum when the mean IoU metric is maximized, its sign is flipped; finally, it is increased by $1$ for easier interpretability.
            \item \textbf{Attention gates:} Later models feature connections between contracting and expanding paths of the same size, which are used to pass information from the same-sized sections of the mode. As \texttt{Attention Gates}\cite{oktay2018attentionunetlearninglook} are used, the model is able to focus on the most important parts of the image, and ignore the rest, providing a large improvement in the result.
            \item \textbf{Deeper, simpler architectures:} While all prior models only contained two encoding and deconding layers, often containing many complex convolutional paths featuring \texttt{Inception} and \texttt{Residual} blocks, for the final models we abandoned this complexity, using simple $3x3$ convolutions in each layer, but increasing the number of layers to $5$. This architecture did no longer benefit from the presence of \texttt{Attention Gates}, which were removed, and was more prone to overfitting, prompting the addition of \texttt{Spatial Dropout} layers, \textit{L2 normalization} in convolutions and change from \texttt{Batch Normalization} to \texttt{Group Normalization} layers. Nevertheless, the higher number of layers and parameters was capable of capturing more of the images' features, resulting in a higher Mean IoU. Another difference is the change in loss function, with the addition of a \texttt{Focal} loss term, weighted with reverse class frequencies and excluding the background, weighted as $20\%$ of the global loss.
            %TODO: add Focal reference
      \end{itemize}

      \section{Failed experiments}
      \label{sec:failed_experiments}
      Other than the experiments mentioned above, we performed many more experiments, which did not lead to any improvement. Chronologically, most experiments are placed after the introduction of \texttt{Attention Gates} and before the introduction of the deeper model, to try to further increase the performance of the model. The main experiments were:
      \begin{itemize}[leftmargin=*]
            \item \textbf{Big rock and background recongnition:} Analyzing the \textit{confusion matrix} of our best model at the time, we noticed that it performed rather well on classes 1 to 3, with a mean IoU computed on just classes close to $0.8$. However, the model seemed to be unable to discriminate the background and big rock classes. Since the training set contained only $63$ images featuring big rocks, the training process was clearly unable to produce a model capable of recognizing them effectively. Therefore, we developed a separate model, with the same structure, but with all labels mapped into only two classes: background (containing all classes except for class 4) and big rock. We attempted to train the model using a weighted \texttt{Categorical Crossentropy} loss function, and the IoU loss, both on the entire training set and only on images containing big rocks. None of the models we produced was however capable of recognizing big rocks significantly better than randomly guessing. We then approched the problem in a different way, still developing a separate model, but this time to recognize the background, with all other classes mapped into the same label, so that big rocks could be recognized as the pixels that are not classified as background by the latter model, nor as any of the other classes by our main model. Sadly, this approach did not work out either.
            \item \textbf{Dilated convolutions:} We tried replacing 5x5 convolutions in our \texttt{Inception} blocks with 3x3 \textit{dilated} convolutions, to reduce the number of parameters of the model while retaining the large \textit{receptive field}, reducing overfitting and incresing the training process's effectiveness. We also experimented with variable \textit{dilation rates}, higher in deeper levels of the network.
            \item \textbf{Model fusion:} Another idea we explored was training multiple models, with either slightly different architectures or simply trained with different seeds, and using them as a sort of \textit{ensamble} method. However, all models' predictions were rather aligned, suggesting models were likely capable of recognizing the same types of features. We tried adding the predictions of our best model to the inputs of a separate model, training the latter with inputs in the form \textit{(image, prior\_prediction)}, both with and without skip connections between the prior predictions and the outputs, with the rationale that this could work as a sort of residual architecture, but without the need for more memory during training.
            \item \textbf{Multiple classification heads:} We considered adding a second \textit{classification head} classifying the outputs of the lower level of the \texttt{Decoder}, trained using downsampled versions of the labels. We attempted training the whole model twice, the first time using a weighted combination of the mean IoU at the coarser level and the mean IoU at the finer level, to incentivise the lower level to learn the needed features, while keeping a baseline performance for the upper level. The second training process was done discaring the added classification head, to maximize the mean IoU at the finer level. We attempted other options as well, including a single training process, adaptively lowering the weight of the mean IoU at the coarser level, and adding a third encoding and deconding level and a third classification head, but did not obtain any significative result.
      \end{itemize}
      Other failed experiments include inserting a \texttt{Transformer} block in the bottleneck layer, using augmentations provided by the KerasCV library, taking care to only use augmentations which preserve local statistics, and using different optimizers from \texttt{Adam} and \texttt{AdamW}.
      %TODO: add transformer reference

      % For your experiments, you might want to present your results in tables.
      % Here's an example of a wide table comparing different models:

      % \begin{table*}[t]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of wide table. Best results are highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\textwidth}{lYYYc}
      %         \toprule
      %         Model            & Accuracy                  & Precision
      %                          & Recall                    & ROC AUC
      %         \\
      %         \midrule
      %         VGG18            & 72.20 $\pm$ 3.06          & 94.95 $\pm$ 0.52
      %                          &
      %         86.95 $\pm$ 0.55 & 80.16 $\pm$ 0.81
      %         \\
      %         Custom Model     & 27.71 $\pm$ 3.19          & 75.70 $\pm$ 1.07
      %                          & 55.75 $\pm$ 2.16          & 36.60 $\pm$ 1.26
      %         \\
      %         ResNet18         & \textbf{89.24 $\pm$ 2.38} & \textbf{95.54
      %         $\pm$ 0.49}      & \textbf{93.43 $\pm$ 1.30} & \textbf{91.68 $\pm$
      %             0.71}
      %         \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tab:Performance}
      % \end{table*}

      % \noindent For more specific measurements, you might use a narrower
      % table:

      % \begin{table}[H]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of table. Best results may be highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\linewidth}{lY}
      %         \toprule
      %         Time [$\mu$s] & Distance [mm] \\
      %         \midrule
      %         22$\pm$4      & 8$\pm$1       \\
      %         17$\pm$3      & 7$\pm$1       \\
      %         15$\pm$3      & 6$\pm$1       \\
      %         13$\pm$2      & 5$\pm$1       \\
      %         10$\pm$2      & 4$\pm$1       \\
      %         8$\pm$2       & 3$\pm$1       \\
      %         5$\pm$1       & 2$\pm$1       \\
      %         37$\pm$1      & 1$\pm$1       \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tb:Measurements}
      % \end{table}

      % \noindent You can also include figures to visualise your results:
      % \begin{figure}[H]
      %     \centering
      %     \includegraphics[width=0.75\linewidth]{random.jpeg}
      %     \caption{Example figure showing [describe what the figure shows]}
      %     \label{fig:results}
      % \end{figure}

      % \noindent Reference figures using like:``As shown in
      % Figure~\ref{fig:results}..."

      % -----------------------------------------------------------------------
      % RESULTS
      % -----------------------------------------------------------------------
      \label{sec:results}
      \section{Results}
      % Present your main findings here. You might want to:
      % \begin{itemize}
      %     \item Compare your results with baselines
      %     \item Highlight key achievements using \textbf{bold text}
      %     \item Explain any unexpected outcomes
      % \end{itemize}

      \begin{table}[H]
            \label{tab:results}
            \centering
            \setlength{\tabcolsep}{3pt}
            \begin{tabularx}{\linewidth}{lY}
                \toprule
                Experiment & Mean IoU \\
                \midrule
                Simple U-Net & 0.38328 \\
                Augmentation & 0.44328 \\
                Batch size reduction & 0.45797 \\
                Custom block & 0.45911 \\
                IoU loss & 0.48808 \\
                Attention gates & 0.59533 \\
                Deeper architectures & 0.62156 \\
                \bottomrule
            \end{tabularx}
            \label{tab:results}
      \end{table}

      Table \ref{tab:results} displays the highest mean IoU on \texttt{DS2} using each of the techniques discussed in Section \ref{sec:succ_experiments}, before introducing the next technique. For each technique, multiple models were tested, for a total of close to $100$ models, most of which tested after introducing all techniques above.

      % -----------------------------------------------------------------------
      % DISCUSSION
      % -----------------------------------------------------------------------
      \section{Discussion}
      % In this section, analyse your results critically. Consider:
      % \begin{itemize}
      %     \item Strengths and weaknesses
      %     \item Limitations and assumptions
      % \end{itemize}

      We developed a deep and robust image segmentation network, capable of performing image segmentation of Mars terrain images, despite the issues with the labeling process described in Section \ref{sec:analysis}. The model captures well the features of three terrain types, with a Mean IoU on said terrains of over $0.8$. The largest weaknesses of the model is the fact that it performs poorly on the big rock class, as it was not capable of learning the class's features from the $63$ images in \texttt{DS1}. A further weakness of the final model is that, having more than $10$ million parameters, it is not trainable on commodity hardware, nor executable on many devices. Regarding this problem, we propose a simpler model, consisting of the best model we were able to develop on commodity hardware, with a Mean IoU on \texttt{DS2} of $0.59533$ and less than $2$ million parameters.

      % -----------------------------------------------------------------------
      % CONTRIBUTIONS
      % -----------------------------------------------------------------------
      \section{Contributions}

      Most ideas and propositions were evenly distributed among the team members, while actual coding was more specific to each member's strengths. The final model and many of the failed experiments are the result of discussing potential improvements and ides between team members.

      % -----------------------------------------------------------------------
      % CONCLUSIONS
      % -----------------------------------------------------------------------
      \section{Conclusions}
      % Summarise your work and discuss potential future directions. This is
      % where you can:
      % \begin{itemize}
      %     \item Restate main contributions
      %     \item Suggest improvements
      %     \item Propose future work
      % \end{itemize}

      % Possible improvements: somehow classify class 4 or refine the model on the other classes.
      TODO: repeat results and state future work.

      In this challenge, we were able to tackle an \textit{image segmentation} task and develop close to $100$ models, exploring the design space of iamge segmentation architectures, with our final model obtaining a Mean IoU on non-background classes of $0.62$ on \texttt{DS2}. Future work centers around the poor performance of the model on the big rock class, for example by training a separate model to recognize big rocks, as explained in Section \ref{sec:failed_experiments}, but with more rock images, obtained or generated in some way. If the attempt were to fail, the class could simply be removed from the model, which would have no effect on the Mean IoU but would solve an undesirable feature of the model, which is the fact that it often miscassifies the background as a big rock, as this has close to no penalty on Mean IoU when the accuracy for big rocks is so low.

      % Remember to include the bibliography!
      \bibliography{references}
      \bibliographystyle{abbrv}

\end{multicols}
\end{document}