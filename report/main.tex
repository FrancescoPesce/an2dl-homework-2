\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}
\usepackage{enumitem}

\title{AN2DL Reports Template}

\begin{document}

\begin{figure}[H]
      \raggedright
      \includegraphics[scale=0.4]{polimi.png} \hfill
      \includegraphics[scale=0.3]{airlab.jpeg}
\end{figure}

\vspace{5mm}

\begin{center}
      % Select between First and Second
      {\Large \textbf{AN2DL - Second Homework Report}}\\
      \vspace{2mm}
      % Change with your Team Name
      {\Large \textbf{LosPollosHermanos}}\\
      \vspace{2mm}
      % Team Members Information
      {\large Mohammadhossein Allahakbari,}
      {\large Michele Miotti,}
      {\large Francesco Pesce}\\
      \vspace{2mm}
      % Codabench Nicknames
      {mh2033,}
      {michelem,}
      {francescopesce}\\
      \vspace{2mm}
      % Matriculation Numbers
      {246639,}
      {249499,}
      {247974}\\
      \vspace{5mm}
      \today
\end{center}
\vspace{5mm}

\begin{multicols}{2}
      % Note: The following sections represent a suggested
      % structure. We don't need to follow it strictly.

      % -----------------------------------------------------------------------
      % INTRODUCTION
      % -----------------------------------------------------------------------
      \section{Introduction}
      % In this section, you should present your project's context and
      % objectives. You might want to:
      % \begin{itemize}
      %     \item Dehe problem (\textit{you may use italics to highlight
      %               definitions})
      %     \item State your goals (\textbf{emphasise key points with bold})
      %     \item Outline your approach
      % \end{itemize}

      % \noindent For instance, you might write: ``This project focuses on
      % \textit{image classification} using \textbf{deep learning} techniques."

      This report presents the results of the \textit{semantic segmentation}\cite{long2015fullyconvolutionalnetworkssemantic}
      task proposed in the second homework of the \texttt{Artificial Neural Networks and Deep Learning} course. Its goal is to classify each pixel in $64\times128$ grayscale images of the Mars surface in one of $5$ classes, each representing a different type of terrain or object: background, soil, bedrock, sand or big rock, similarly to \cite{li2024marssegmarssurfacesemantic}.

      % -----------------------------------------------------------------------
      % PROBLEM ANALYSIS
      % -----------------------------------------------------------------------
      \section{Problem Analysis}
      \label{sec:analysis}
      % Here you can discuss your initial analysis of the problem. Consider
      % including:
      % \begin{enumerate}
      %     % 8 classes, 96x96 rgb images, labels, etc
      %     \item Dataset characteristics
      %     \item Main challenges % The test set is horrible
      %     That the test set was not horrible? Is this what they mean?
      %     \item Initial assumptions 
      % \end{enumerate}

      % \noindent If you need to reference papers, use the citation command:
      % Recent work~\cite{lecun2015deep} suggests..."

      We were provided two separate datasets, containing around $2500$ and $10000$ images respectively, which we modelled as generated by the same distribution. The former, referred to as \texttt{DS1}, contained \textit{labeled} images, and was used to train the model, while the latter was unlabeled, and was instead used for evaluation, by uploading the models' predictions to the \texttt{Kaggle}\cite{kaggle} platform. This dataset was further split into two subsets, \texttt{DS2} and \texttt{DS3}, with the former containing about $25\%$ of the data and being used to provide public evaluations during the competition, while the latter being used for the final evaluation. After manually analyzing \texttt{DS1}, we found and discarded $110$ images doctored with unraled \textit{overlays}. Moreover, analyzing its labels, we found them to be rather imprecise, as they were labeled by hand. This lack of precision can be modelled as a contribution of a \textit{bias} in the labelling process, caused by the same type of imprecisions being repeated in a consistent way, and of an added \textit{variability} of the dataset. We found no need to relabel the images in \texttt{DS1}, as a rich model should be capable of learning the bias of the labelling process, while the added variability in \texttt{DS3} cannot be removed by any process.

      Given an image segmentation model, for each class $i$, we define $A_i$ as the set of labels belonging to class $i$ and $B_i$ as the set of labels the model classifies as belonging to class $i$. The \textit{Intersection over Union} (IoU) metric for class $i$ is defined as: $$\text{IoU}_i := \dfrac{|A_i \cap B_i|}{|A_i \cup B_i|}$$ and models are evaluated based on the \textit{Mean IoU} (MIoU) metric, defined as the mean of the IoUs over all non-background classses.

      % -----------------------------------------------------------------------   
      % METHOD
      % -----------------------------------------------------------------------
      \section{Method}
      % Not sure what they are asking here. The final model?
      % This section should detail your approach. You can use equations to
      % explain your methodology. For example, a simple model representation:
      % \begin{equation}
      %     \label{eq:model}
      %     f(x) = \text{softmax}(Wx + b)
      % \end{equation}

      % \noindent Or a more complex loss function:
      % \begin{equation}
      %     \label{eq:loss}
      %     \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} y_i\log(\hat{y}_i)
      % \end{equation}

      % \noindent Reference these equations in your text, like:``As shown in
      % equation~\ref{eq:model}..."

      The development process centers around \texttt{Jupyter} notebooks, using the \texttt{Tensorflow}\cite{TensorFlow} and \texttt{Keras}\cite{chollet2015keras} libraries for \texttt{Python}, mostly run locally using an \texttt{NVIDIA RTX 2060 Mobile}, with the exception of very deep models, which were trained on \texttt{Kaggle} due to hardware limitations.

      To reduce \textit{overfitting}, we split the training set into a training and a \textit{validation} set, containing $10-20\%$ of the data. Differently from the first homework, we noticed that validation and \texttt{DS2} MIoUs were similar, so we were able to avoid submitting models with low validation MIoU.

      All developed models share the same basic structure, displayed in Figure \ref{fig:model_structure}. It is heavily inspired by the U-Net\cite{ronneberger2015unetconvolutionalnetworksbiomedical} architecture and capable of capturing both \textit{local information} and the \textit{global context}. The \textit{contracting path} is composed of a stack of $k$ convolutional and downsampling layers, where each layer halves the spatial dimensions of the input and doubles the number of channels. The \textit{expanding path} is composed of a stack of convolutional and upsampling layers, with inputs the outputs of the previous layer and of the contracting layer with the same spatial dimensions. They act as the opposite of the former layers, doubling the spatial dimensions and halving the number of channels. 

      \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\linewidth]{model-structure.png}
            \caption{The basic structure of our models, shown here with $k$ layers. On the left, the contracting or \texttt{Downsampling} path, in the middle, the \texttt{Bottleneck}, and on the right the expanding or \texttt{Upsampling} path.}
            \label{fig:model_structure}
      \end{figure}

      % -----------------------------------------------------------------------
      % EXPERIMENTS
      % -----------------------------------------------------------------------
      \section{Experiments}
      \label{sec:experiments}
      During development, we tested around $100$ different models, with our experiments being broadly categorizable in chronological order as:
      \begin{itemize}[leftmargin=*]
            \setlength\itemsep{0em}
            \item \textbf{Initial experiments:} We started development with a simple U-Net-like architecture with $k=2$, then introduced simple image \textit{augmentations} such as flips, zooms and changes in brightness. Spatial augmentations were applied to the labels as well, so that they'd refer to the transformed version of their own pixel. Our first obstacle was the model's inability to learn consistently, as minor changes would result in the training processes halting at a MIoU lower then $0.1$. The issue was likely the fact that the training process halted in a \textit{local minimum}, and the more \textit{noisy gradient estimations} produced by a lower \textit{batch size} were able to escape said minimum, allowing us to use larger networks.
            \item \textbf{Model M1:} By following our experience with the first homework, we replaced $3\times3$ convolutions with our custom block (described in detail in the previous report), which is formed by a stack of \texttt{Inception} blocks and residual connections. Moreover, to directly optimize the model for the evaluation, we decided to implement a loss function based on MIoU, by directly accessing the backend from \texttt{Keras}. Since it needs to be \textit{differentiable}, it is computed using the posterior probabilities estimated by the model instead of just their maximum, and its sign is flipped to use \textit{gradient descent}. Moreover, we inserted \texttt{Attention Gates}\cite{oktay2018attentionunetlearninglook} between contracting and expanding paths of the same size, to guide the model to learn the most important features of the image, and carefully analyzed the complex \textit{design space} for model architecture and hyperparameters. These changes resulted in a very fine-tuned model with $1.8$ million parameters, and a MIoU on \texttt{DS2} of $0.595$, trained using \texttt{AdamW} with initial learning rate $10^{-4}$, adaptively lowered during training based on validation loss.
            \item \textbf{Failed experiments:} We performed many failed experiments based on M1's architecture, spending by far the most development time in this phase. These experiments include replacing $5\times5$ convolutions with $3\times3$ \textit{dilated} convolutions to mantain the large \textit{receptive field} while reducing the number of parameters, inserting a \texttt{Transformer}\cite{vaswani2023attentionneed} block in the bottleneck layer, using augmentations provided by the \texttt{KerasCV}\cite{chollet2015keras} library, taking care to only use augmentations which preserve \textit{local statistics}, and using different optimizers. We considered adding a second \textit{classification head} classifying the outputs of the intermediate layer of the contracting path, trained using downsampled versions of the labels, and inserting its MIoU in the loss function, to incentivise the lower level to learn the desired features, experimenting with many weighting techniques, including adaptive ones. We also tried inserting the predictions from M1 into the inputs of a separate model, training the latter with inputs in the form \textit{(image, M1\_prediction)}, with skip connections between M1 predictions and the outputs, with the rationale that this could work as a sort of residual architecture, but without the need for more memory during training. Finally, we experimented with separate models for background and big rock recognition, as discussed in Section \ref{sec:conclusions}.
            \item \textbf{Model M2:} Model M2 differs from prior models in two key aspects: it uses simpler $3\times3$ convolutions in each layer instead of stacks of custom blocks, and the number of layers in the contracting and expanding paths is $k=5$, with upsampling performed with \textit{fractional stride} convolutions. This architecture has around $99$ million parameters, making it more prone to overfitting and prompting the addition of \texttt{Spatial Dropout} layers, \textit{L2 normalization} in convolutions and the change from \texttt{Batch Normalization} to \texttt{Group Normalization} layers. Since the architecture no longer benefits from the presence of \texttt{Attention Gates}, they were removed. A \texttt{Focal}\cite{lin2018focallossdenseobject} loss term, using inverse class frequency weights excluding the background, was added to the global loss, with a weight of $0.2$. M2 is capable of capturing more of the images' features, resulting in a higher MIoU of $0.629$ on \texttt{DS2}.
      \end{itemize}

      % For your experiments, you might want to present your results in tables.
      % Here's an example of a wide table comparing different models:

      % \begin{table*}[t]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of wide table. Best results are highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\textwidth}{lYYYc}
      %         \toprule
      %         Model            & Accuracy                  & Precision
      %                          & Recall                    & ROC AUC
      %         \\
      %         \midrule
      %         VGG18            & 72.20 $\pm$ 3.06          & 94.95 $\pm$ 0.52
      %                          &
      %         86.95 $\pm$ 0.55 & 80.16 $\pm$ 0.81
      %         \\
      %         Custom Model     & 27.71 $\pm$ 3.19          & 75.70 $\pm$ 1.07
      %                          & 55.75 $\pm$ 2.16          & 36.60 $\pm$ 1.26
      %         \\
      %         ResNet18         & \textbf{89.24 $\pm$ 2.38} & \textbf{95.54
      %         $\pm$ 0.49}      & \textbf{93.43 $\pm$ 1.30} & \textbf{91.68 $\pm$
      %             0.71}
      %         \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tab:Performance}
      % \end{table*}

      % \noindent For more specific measurements, you might use a narrower
      % table:

      % \begin{table}[H]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of table. Best results may be highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\linewidth}{lY}
      %         \toprule
      %         Time [$\mu$s] & Distance [mm] \\
      %         \midrule
      %         22$\pm$4      & 8$\pm$1       \\
      %         17$\pm$3      & 7$\pm$1       \\
      %         15$\pm$3      & 6$\pm$1       \\
      %         13$\pm$2      & 5$\pm$1       \\
      %         10$\pm$2      & 4$\pm$1       \\
      %         8$\pm$2       & 3$\pm$1       \\
      %         5$\pm$1       & 2$\pm$1       \\
      %         37$\pm$1      & 1$\pm$1       \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tb:Measurements}
      % \end{table}

      % \noindent You can also include figures to visualise your results:
      % \begin{figure}[H]
      %     \centering
      %     \includegraphics[width=0.75\linewidth]{random.jpeg}
      %     \caption{Example figure showing [describe what the figure shows]}
      %     \label{fig:results}
      % \end{figure}

      % \noindent Reference figures using like:``As shown in
      % Figure~\ref{fig:results}..."

      % -----------------------------------------------------------------------
      % RESULTS
      % -----------------------------------------------------------------------
      \label{sec:results}
      \section{Results}
      % Present your main findings here. You might want to:
      % \begin{itemize}
      %     \item Compare your results with baselines
      %     \item Highlight key achievements using \textbf{bold text}
      %     \item Explain any unexpected outcomes
      % \end{itemize}

      Table \ref{tab:results} displays the highest MIoU on \texttt{DS2} using the succesful techniques discussed in Section \ref{sec:experiments}, before introducing the next technique.

      \begin{table}[H]
            \centering
            \setlength{\tabcolsep}{3pt}
            \begin{tabularx}{\linewidth}{lY}
                \toprule
                Experiment & MIoU \\
                \midrule
                Simple U-Net & 0.38328 \\
                Augmentation & 0.44328 \\
                Custom block & 0.45911 \\
                IoU loss & 0.48808 \\
                Attention gates (M1) & 0.59533 \\
                Deeper architectures (M2) & 0.62900 \\
                \bottomrule
            \end{tabularx}
            \label{tab:results}
            \caption{Experimental results on \texttt{DS2}.}
      \end{table}


      % -----------------------------------------------------------------------
      % DISCUSSION
      % -----------------------------------------------------------------------
      \section{Discussion}
      % In this section, analyse your results critically. Consider:
      % \begin{itemize}
      %     \item Strengths and weaknesses
      %     \item Limitations and assumptions
      % \end{itemize}

      Despite the issues of the labeling process described in Section \ref{sec:analysis}, we were able to develop a deep and robust image segmentation network for Mars terrain images by using techniques from the state of the art and our own intuitions. 
      
      The \textit{confusion matrix} for the validation set indicates that M2 captures terrain features well, with a MIoU of over $0.8$ on the three terrain classes. Conversely, its largest weakness is the fact that it performs poorly at classifying big rocks, as it was unable to learn the class's features from the $63$ images in \texttt{DS1} that feature them. 
      
      The current industry trend of integrating AI models in mobile and less performing devices highlights another weakness of M2, as it cannot be used for prediction in many such devices. For this purpose we propose M1, which has a MIoU penalty of around $3\%$, but was trained on a commodity GPU and has fewer than $2\%$ the number of parameters.

      % -----------------------------------------------------------------------
      % CONTRIBUTIONS
      % -----------------------------------------------------------------------
      \section{Contributions}

      Most ideas and propositions were evenly distributed among the team members, while actual coding was more specific to each member's strengths. The final models and many of the failed experiments are the result of discussing potential improvements and ideas between team members.

      % -----------------------------------------------------------------------
      % CONCLUSIONS
      % -----------------------------------------------------------------------
      \section{Conclusions}
      \label{sec:conclusions}
      % Summarise your work and discuss potential future directions. This is
      % where you can:
      % \begin{itemize}
      %     \item Restate main contributions
      %     \item Suggest improvements
      %     \item Propose future work
      % \end{itemize}

      In this challenge, we were able to tackle an \textit{image segmentation} task and develop close to $100$ models, with the final model having a $0.629$ MIoU on \texttt{DS2}. Future work centers around the classification of big rocks. An idea we wish to further explore is developing separate models for big rock or background recognition. Despite having experimented with both, considering both the options of using the full dataset and just the images containing big rocks, and having obtained no results, we believe that coupling the idea with a richer dataset or with advanced image augmentation techniques to generate more images could provide meanigful results, which would hugely impact the model's MIoU.

      % Remember to include the bibliography!
      \bibliography{references}
      \bibliographystyle{abbrv}

\end{multicols}
\end{document}